{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import sys\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pycocotools.mask as maskUtils\n",
    "\n",
    "from matting_api import Matting\n",
    "\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "import inference as infer\n",
    "from datasets import COCOADataset, read_COCOA\n",
    "from demo_utils import *\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'size'   : 16}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None if out of bounds, otherwise return coord\n",
    "def clamp_or_none(coord, R, C):\n",
    "    r, c = coord\n",
    "    return coord if (r >= 0 and r < R) and (c >= 0 and c < C) else None\n",
    "\n",
    "# [UpperLeft, Top, UpperRight, Left, Right, BottomLeft, Bottom, BottomRight]\n",
    "def neighbors(r, c, R, C, off=3):\n",
    "    lst = [(r - off, c - off), (r - off, c), (r - off, c + off), (r, c - off), \n",
    "           (r, c + off), (r + off, c - off), (r + off, c), (r + off, c + off)]\n",
    "    return [clamp_or_none(coord, R, C) for coord in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def pad_mask(mask, size=9):\n",
    "    kernel = np.ones((size, size))\n",
    "    return cv2.filter2D(mask, 1, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [1:30:05<00:00,  2.16s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "channels = 8\n",
    "\n",
    "phase = 'train'\n",
    "root_dict = {'train': \"../data/COCOA/train2014\", 'val': \"../data/COCOA/val2014\"}\n",
    "\n",
    "img_root = root_dict[phase]\n",
    "annot_path = \"../data/COCOA/annotations/COCO_amodal_{}2014.json\".format(phase)\n",
    "\n",
    "output_root = \"../data/COCOA/pixel_graphs_{}/\".format(phase)\n",
    "\n",
    "data_reader = COCOADataset(annot_path)\n",
    "img_count = len(data_reader.annot_info)\n",
    "\n",
    "for img_idx in tqdm(range(img_count)):\n",
    "    modal, category, ori_bboxes, amodal_gt, image_fn = data_reader.get_image_instances(\n",
    "        img_idx, with_gt=True, ignore_stuff=True)\n",
    "    \n",
    "    modal_count = len(modal)\n",
    "    gt_order_matrix = infer.infer_gt_order(modal, amodal_gt)\n",
    "    \n",
    "    image_fn = os.path.join(img_root, image_fn)\n",
    "    img = Image.open(image_fn)\n",
    "    height, width = img.height, img.width\n",
    "    image = np.array(img)\n",
    "    \n",
    "    # Construct modals and graph\n",
    "    modals = np.zeros((height, width))\n",
    "    for i in range(modal_count):\n",
    "        modals += (i + 1) * (modal[i] > 0)\n",
    "    graph = np.zeros((height, width, channels))\n",
    "\n",
    "    for r in range(height):\n",
    "        for c in range(width):\n",
    "            pairs = neighbors(r, c, height, width)\n",
    "            for k in range(channels):\n",
    "                pair = pairs[k]\n",
    "                if not pair:\n",
    "                    continue\n",
    "                u, v = pair\n",
    "                i, j = int(modals[r, c]), int(modals[u, v])\n",
    "                if not i and j:\n",
    "                    graph[r, c, k] = -1\n",
    "                elif not j and i:\n",
    "                    graph[r, c, k] = 1\n",
    "                elif not j and not i:\n",
    "                    graph[r, c, k] = 0\n",
    "                else:\n",
    "                    graph[r, c, k] = gt_order_matrix[i - 1, j - 1]\n",
    "    \n",
    "    # Save graph\n",
    "    path = os.path.join(output_root, str(img_idx) + '_graph')\n",
    "    tensor = torch.tensor(graph)\n",
    "    torch.save(tensor, path)\n",
    "    \n",
    "    # Save mask\n",
    "    mask = pad_mask(1 * (modals > 0)) > 0\n",
    "    path = os.path.join(output_root, str(img_idx) + '_mask')\n",
    "    tensor = torch.tensor(mask)\n",
    "    torch.save(tensor, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
