{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3686,
     "status": "ok",
     "timestamp": 1618023115677,
     "user": {
      "displayName": "Briana Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjdwetoa6LIfFqOnjqFLSYVcVF68sC1Q7rHXKnLFA=s64",
      "userId": "10858716679377069069"
     },
     "user_tz": 300
    },
    "id": "yvOVHVVIZWnf",
    "outputId": "b6d37905-f1da-4481-adf3-3b8866a83ed1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /home/brizhanga/.conda/envs/spatial-seg-env/lib/python3.8/site-packages (3.2.1)\r\n",
      "Requirement already satisfied: numpy>=1.17.5 in /home/brizhanga/.conda/envs/spatial-seg-env/lib/python3.8/site-packages (from h5py) (1.19.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vsRGyGSfVYMB"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wzmE6yXkuQ3X"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=9, init_features=32):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        features = init_features\n",
    "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(\n",
    "            features * 16, features * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose2d(\n",
    "            features * 2, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        \n",
    "        dec0 = self.conv(dec1)\n",
    "        return torch.sigmoid(dec0[0]), torch.tanh(dec0[1:])\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        name + \"conv1\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=in_channels,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
    "                    (\n",
    "                        name + \"conv2\",\n",
    "                        nn.Conv2d(\n",
    "                            in_channels=features,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
    "                ]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1618023896816,
     "user": {
      "displayName": "Briana Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjdwetoa6LIfFqOnjqFLSYVcVF68sC1Q7rHXKnLFA=s64",
      "userId": "10858716679377069069"
     },
     "user_tz": 300
    },
    "id": "gu1pzRrqvR65",
    "outputId": "ad34545b-8195-492b-da68-f4e564c484bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (encoder1): Sequential(\n",
       "    (enc1conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (enc1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc1relu1): ReLU(inplace=True)\n",
       "    (enc1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (enc1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc1relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder2): Sequential(\n",
       "    (enc2conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (enc2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc2relu1): ReLU(inplace=True)\n",
       "    (enc2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (enc2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc2relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder3): Sequential(\n",
       "    (enc3conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (enc3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc3relu1): ReLU(inplace=True)\n",
       "    (enc3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (enc3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc3relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder4): Sequential(\n",
       "    (enc4conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (enc4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc4relu1): ReLU(inplace=True)\n",
       "    (enc4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (enc4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (enc4relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bottleneck): Sequential(\n",
       "    (bottleneckconv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bottlenecknorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bottleneckrelu1): ReLU(inplace=True)\n",
       "    (bottleneckconv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bottlenecknorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bottleneckrelu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (upconv4): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (decoder4): Sequential(\n",
       "    (dec4conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (dec4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec4relu1): ReLU(inplace=True)\n",
       "    (dec4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (dec4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec4relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (decoder3): Sequential(\n",
       "    (dec3conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (dec3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec3relu1): ReLU(inplace=True)\n",
       "    (dec3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (dec3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec3relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (decoder2): Sequential(\n",
       "    (dec2conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (dec2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec2relu1): ReLU(inplace=True)\n",
       "    (dec2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (dec2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec2relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (upconv1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (decoder1): Sequential(\n",
       "    (dec1conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (dec1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec1relu1): ReLU(inplace=True)\n",
       "    (dec1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (dec1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (dec1relu2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv): Conv2d(32, 9, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = UNet()\n",
    "cuda_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(cuda_device)\n",
    "net.to(cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ckMxy3FdVqZP"
   },
   "outputs": [],
   "source": [
    "crit1 = nn.CrossEntropyLoss()\n",
    "crit2 = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-05)\n",
    "\n",
    "# TODO: add scheduler\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[epochs to drop], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNmwMXZPnjG8"
   },
   "outputs": [],
   "source": [
    "t_losses = []\n",
    "v_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "executionInfo": {
     "elapsed": 434,
     "status": "error",
     "timestamp": 1618023827115,
     "user": {
      "displayName": "Briana Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjdwetoa6LIfFqOnjqFLSYVcVF68sC1Q7rHXKnLFA=s64",
      "userId": "10858716679377069069"
     },
     "user_tz": 300
    },
    "id": "oSCHYwQVnkgf",
    "outputId": "076f8e2b-4515-4571-ec1f-bcaf38f508d3"
   },
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-94a797e3f5bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test1/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'test1/'"
     ]
    }
   ],
   "source": [
    "folder = f'../tests/lr{lr}_momentum{momentum}_weight_decay{weight_decay}/'\n",
    "os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s6TLNlUqnwZl",
    "outputId": "d4ba41f9-b608-454c-a2fb-aa446942d997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0004069805145263672 seconds have passed\n",
      "creating test1/0.pth\n",
      "validation loss: 5.045575203960889\n",
      "1\n",
      "124.80121755599976 seconds have passed\n",
      "creating test1/1.pth\n",
      "validation loss: 5.033679953176681\n",
      "2\n",
      "257.0098695755005 seconds have passed\n",
      "creating test1/2.pth\n",
      "validation loss: 5.051197448821917\n",
      "3\n",
      "389.3965401649475 seconds have passed\n",
      "creating test1/3.pth\n",
      "validation loss: 5.019236014313893\n",
      "4\n",
      "521.7518222332001 seconds have passed\n",
      "creating test1/4.pth\n",
      "validation loss: 5.0681704821651925\n",
      "5\n",
      "654.0307619571686 seconds have passed\n",
      "creating test1/5.pth\n",
      "validation loss: 5.013479650837102\n",
      "6\n",
      "786.3195731639862 seconds have passed\n",
      "creating test1/6.pth\n",
      "validation loss: 5.105839188784769\n",
      "7\n",
      "918.6307651996613 seconds have passed\n",
      "creating test1/7.pth\n",
      "validation loss: 5.043568489486224\n",
      "8\n",
      "1050.9152693748474 seconds have passed\n",
      "creating test1/8.pth\n",
      "validation loss: 5.034535386790968\n",
      "9\n",
      "1183.2692708969116 seconds have passed\n",
      "creating test1/9.pth\n",
      "validation loss: 5.023528079464011\n",
      "10\n",
      "1315.5202684402466 seconds have passed\n",
      "creating test1/10.pth\n",
      "validation loss: 5.018871097532037\n",
      "11\n",
      "1447.7544071674347 seconds have passed\n",
      "creating test1/11.pth\n",
      "validation loss: 5.031180091916698\n",
      "12\n",
      "1580.0370013713837 seconds have passed\n",
      "creating test1/12.pth\n",
      "validation loss: 5.018100563793967\n",
      "13\n",
      "1712.290522813797 seconds have passed\n",
      "creating test1/13.pth\n",
      "validation loss: 5.016325570132635\n",
      "14\n",
      "1844.4157445430756 seconds have passed\n",
      "creating test1/14.pth\n",
      "validation loss: 5.0065025074841225\n",
      "15\n",
      "1976.5957164764404 seconds have passed\n",
      "creating test1/15.pth\n",
      "validation loss: 5.028742764094105\n",
      "16\n",
      "2108.7890074253082 seconds have passed\n",
      "creating test1/16.pth\n",
      "validation loss: 5.011654267572377\n",
      "17\n",
      "2240.9840173721313 seconds have passed\n",
      "creating test1/17.pth\n",
      "validation loss: 5.063551065039961\n",
      "18\n",
      "2373.0902123451233 seconds have passed\n",
      "creating test1/18.pth\n",
      "validation loss: 5.024084149974666\n",
      "19\n",
      "2505.24254488945 seconds have passed\n",
      "creating test1/19.pth\n",
      "validation loss: 5.033242155427802\n",
      "20\n",
      "2637.46732378006 seconds have passed\n",
      "creating test1/20.pth\n",
      "validation loss: 5.004807802912307\n",
      "21\n",
      "2769.7357172966003 seconds have passed\n",
      "creating test1/21.pth\n",
      "validation loss: 5.021293297205886\n",
      "22\n",
      "2902.0439958572388 seconds have passed\n",
      "creating test1/22.pth\n",
      "validation loss: 5.0076443757096385\n",
      "23\n",
      "3034.2720828056335 seconds have passed\n",
      "creating test1/23.pth\n",
      "validation loss: 5.027826575383748\n",
      "24\n",
      "3166.5151443481445 seconds have passed\n",
      "creating test1/24.pth\n",
      "validation loss: 5.020945563708266\n",
      "25\n",
      "3298.79389500618 seconds have passed\n",
      "creating test1/25.pth\n",
      "validation loss: 5.045163724520435\n",
      "26\n",
      "3430.94744181633 seconds have passed\n",
      "creating test1/26.pth\n",
      "validation loss: 5.024782558010049\n",
      "27\n",
      "3563.168437242508 seconds have passed\n",
      "creating test1/27.pth\n",
      "validation loss: 5.032724756084076\n",
      "28\n",
      "3695.3261268138885 seconds have passed\n",
      "creating test1/28.pth\n",
      "validation loss: 5.01893958163588\n",
      "29\n",
      "3827.535614967346 seconds have passed\n",
      "creating test1/29.pth\n",
      "validation loss: 5.018682383511164\n",
      "30\n",
      "3959.6965799331665 seconds have passed\n",
      "creating test1/30.pth\n",
      "validation loss: 5.020408976567935\n",
      "31\n",
      "4091.8166501522064 seconds have passed\n",
      "creating test1/31.pth\n",
      "validation loss: 5.03215656051897\n",
      "32\n",
      "4223.988304376602 seconds have passed\n",
      "creating test1/32.pth\n",
      "validation loss: 5.066561166554282\n",
      "33\n",
      "4356.026305437088 seconds have passed\n",
      "creating test1/33.pth\n",
      "validation loss: 5.015956612482463\n",
      "34\n",
      "4488.08104968071 seconds have passed\n",
      "creating test1/34.pth\n",
      "validation loss: 5.036598976344278\n",
      "35\n",
      "4620.161002874374 seconds have passed\n",
      "creating test1/35.pth\n",
      "validation loss: 5.044158732237881\n",
      "36\n",
      "4752.317080259323 seconds have passed\n",
      "creating test1/36.pth\n",
      "validation loss: 5.018938805958996\n",
      "37\n",
      "4884.371451616287 seconds have passed\n",
      "creating test1/37.pth\n",
      "validation loss: 5.032201848617972\n",
      "38\n",
      "5016.456577777863 seconds have passed\n",
      "creating test1/38.pth\n",
      "validation loss: 5.081222632159926\n",
      "39\n",
      "5148.548524856567 seconds have passed\n",
      "creating test1/39.pth\n",
      "validation loss: 5.014151591144196\n",
      "40\n",
      "5280.727460861206 seconds have passed\n",
      "creating test1/40.pth\n",
      "validation loss: 5.054928635897702\n",
      "41\n",
      "5412.870637655258 seconds have passed\n",
      "creating test1/41.pth\n",
      "validation loss: 5.039270432028052\n",
      "42\n",
      "5545.13329410553 seconds have passed\n",
      "creating test1/42.pth\n",
      "validation loss: 5.019452555538857\n",
      "43\n",
      "5677.248375654221 seconds have passed\n",
      "creating test1/43.pth\n",
      "validation loss: 5.039089507436099\n",
      "44\n",
      "5809.467837095261 seconds have passed\n",
      "creating test1/44.pth\n",
      "validation loss: 5.008196616825992\n",
      "45\n",
      "5941.532888412476 seconds have passed\n",
      "creating test1/45.pth\n",
      "validation loss: 5.0311717366519035\n",
      "46\n",
      "6073.774037361145 seconds have passed\n",
      "creating test1/46.pth\n",
      "validation loss: 5.03172259951291\n",
      "47\n",
      "6205.8633415699005 seconds have passed\n",
      "creating test1/47.pth\n",
      "validation loss: 5.066498980130235\n",
      "48\n",
      "6338.0214676856995 seconds have passed\n",
      "creating test1/48.pth\n",
      "validation loss: 5.057582054236164\n",
      "49\n",
      "6470.132466316223 seconds have passed\n",
      "creating test1/49.pth\n",
      "validation loss: 5.028952756973162\n",
      "50\n",
      "6602.314874887466 seconds have passed\n",
      "creating test1/50.pth\n",
      "validation loss: 5.009279669147649\n",
      "51\n",
      "6734.433314085007 seconds have passed\n",
      "creating test1/51.pth\n",
      "validation loss: 5.000129630304363\n",
      "52\n",
      "6866.6583960056305 seconds have passed\n",
      "creating test1/52.pth\n",
      "validation loss: 5.044791851141682\n",
      "53\n",
      "6998.797549009323 seconds have passed\n",
      "creating test1/53.pth\n",
      "validation loss: 5.016107052972872\n",
      "54\n",
      "7130.967816114426 seconds have passed\n",
      "creating test1/54.pth\n",
      "validation loss: 5.032046140056767\n",
      "55\n",
      "7263.092307806015 seconds have passed\n",
      "creating test1/55.pth\n",
      "validation loss: 5.026522768686895\n",
      "56\n",
      "7395.239781856537 seconds have passed\n",
      "creating test1/56.pth\n",
      "validation loss: 5.089231930366934\n",
      "57\n",
      "7527.344665527344 seconds have passed\n",
      "creating test1/57.pth\n",
      "validation loss: 5.016112628048414\n",
      "58\n",
      "7659.501630306244 seconds have passed\n",
      "creating test1/58.pth\n",
      "validation loss: 5.010133921283565\n",
      "59\n",
      "7791.670941829681 seconds have passed\n",
      "creating test1/59.pth\n",
      "validation loss: 5.153710069721693\n",
      "60\n",
      "7923.948273181915 seconds have passed\n",
      "creating test1/60.pth\n",
      "validation loss: 5.03466310239818\n",
      "61\n",
      "8056.12908244133 seconds have passed\n",
      "creating test1/61.pth\n",
      "validation loss: 5.075989432530861\n",
      "62\n",
      "8188.298406839371 seconds have passed\n",
      "creating test1/62.pth\n",
      "validation loss: 5.054081601639316\n",
      "63\n",
      "8320.562765836716 seconds have passed\n",
      "creating test1/63.pth\n",
      "validation loss: 5.047407015545727\n",
      "64\n",
      "8452.846245527267 seconds have passed\n",
      "creating test1/64.pth\n",
      "validation loss: 5.022019837000599\n",
      "65\n",
      "8584.94543004036 seconds have passed\n",
      "creating test1/65.pth\n",
      "validation loss: 5.027362660185932\n",
      "66\n",
      "8717.128024101257 seconds have passed\n",
      "creating test1/66.pth\n",
      "validation loss: 5.068359157810472\n",
      "67\n",
      "8849.274763584137 seconds have passed\n",
      "creating test1/67.pth\n",
      "validation loss: 5.051235043839233\n",
      "68\n",
      "8981.503824472427 seconds have passed\n",
      "creating test1/68.pth\n",
      "validation loss: 5.02197762548107\n",
      "69\n",
      "9113.729490756989 seconds have passed\n",
      "creating test1/69.pth\n",
      "validation loss: 5.0143445223978125\n",
      "70\n",
      "9246.046629667282 seconds have passed\n",
      "creating test1/70.pth\n",
      "validation loss: 5.049059900518966\n",
      "71\n",
      "9378.297849416733 seconds have passed\n",
      "creating test1/71.pth\n",
      "validation loss: 5.007112700645238\n",
      "72\n",
      "9510.555926561356 seconds have passed\n",
      "creating test1/72.pth\n",
      "validation loss: 5.023485152688745\n",
      "73\n",
      "9642.74703335762 seconds have passed\n",
      "creating test1/73.pth\n",
      "validation loss: 5.037625125009719\n",
      "74\n",
      "9774.897763967514 seconds have passed\n",
      "creating test1/74.pth\n",
      "validation loss: 5.052765580072795\n",
      "75\n",
      "9907.030029773712 seconds have passed\n",
      "creating test1/75.pth\n",
      "validation loss: 5.042798159873649\n",
      "76\n",
      "10039.29110622406 seconds have passed\n",
      "creating test1/76.pth\n",
      "validation loss: 5.009495982568558\n",
      "77\n",
      "10171.445785284042 seconds have passed\n",
      "creating test1/77.pth\n",
      "validation loss: 5.018809645143274\n",
      "78\n",
      "10303.561432123184 seconds have passed\n",
      "creating test1/78.pth\n",
      "validation loss: 5.004933646280471\n",
      "79\n",
      "10435.805084943771 seconds have passed\n",
      "creating test1/79.pth\n",
      "validation loss: 5.026150975325336\n",
      "80\n",
      "10567.957792758942 seconds have passed\n",
      "creating test1/80.pth\n",
      "validation loss: 5.069965545445273\n",
      "81\n",
      "10700.232388973236 seconds have passed\n",
      "creating test1/81.pth\n",
      "validation loss: 5.050659153559437\n",
      "82\n",
      "10832.421431303024 seconds have passed\n",
      "creating test1/82.pth\n",
      "validation loss: 5.085928667081546\n",
      "83\n",
      "10964.63387966156 seconds have passed\n",
      "creating test1/83.pth\n",
      "validation loss: 5.0213037059731676\n",
      "84\n",
      "11096.761098384857 seconds have passed\n",
      "creating test1/84.pth\n",
      "validation loss: 5.029818807562736\n",
      "85\n",
      "11228.943835735321 seconds have passed\n",
      "creating test1/85.pth\n",
      "validation loss: 5.002572354796815\n",
      "86\n",
      "11361.166717290878 seconds have passed\n",
      "creating test1/86.pth\n",
      "validation loss: 5.0255216964303635\n",
      "87\n",
      "11493.348385810852 seconds have passed\n",
      "creating test1/87.pth\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "beg = 0\n",
    "end = 100\n",
    "\n",
    "# if not os.path.exists(folder):\n",
    "#     t_losses = []\n",
    "#     v_losses = []\n",
    "# else:\n",
    "#     t_losses = np.loadtxt(folder + 't_losses.txt', delimiter=',')\n",
    "#     v_losses = np.loadtxt(folder + 'v_losses.txt', delimiter=',')\n",
    "\n",
    "for epoch in range(beg, end):  # loop over the dataset multiple times\n",
    "    print(epoch)\n",
    "    print(str(time.time() - start) + ' seconds have passed')\n",
    "    \n",
    "    path = folder + str(epoch) + '.pth'\n",
    "    new = True\n",
    "    if os.path.exists(path):\n",
    "        net.load_state_dict(torch.load(path))\n",
    "        new = False\n",
    "    else:\n",
    "        print('creating ' + path)\n",
    "    \n",
    "    t_loss = 0\n",
    "    t_count = 0\n",
    "    v_loss = 0\n",
    "    v_count = 0\n",
    "    for i, data in enumerate(nyu_train):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, depths = data\n",
    "        inputs, depths = inputs.to(cuda_device), depths.to(cuda_device)\n",
    "\n",
    "        if new:\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, depths)\n",
    "        \n",
    "        if new: \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        t_loss += loss.detach().cpu().numpy()\n",
    "        t_count += 1\n",
    "    if epoch >= len(t_losses):\n",
    "        t_losses.append([0])\n",
    "    t_losses[epoch] = t_loss/t_count\n",
    "    \n",
    "    for i, data in enumerate(nyu_val):\n",
    "        inputs, depths = data\n",
    "        inputs, depths = inputs.to(cuda_device), depths.to(cuda_device)\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, depths)\n",
    "        \n",
    "        v_loss += loss.detach().cpu().numpy()\n",
    "        v_count += 1\n",
    "    if epoch >= len(v_losses):\n",
    "        v_losses.append([0])\n",
    "    v_losses[epoch] = v_loss/v_count\n",
    "    print('validation loss:', v_loss/v_count)\n",
    "    \n",
    "    torch.save(net.state_dict(), path)\n",
    "\n",
    "print('Finished Training')\n",
    "print('Took ' + str((time.time() - start)/60) + ' minutes')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNupobF3Cu0yqdBpqggUlNJ",
   "collapsed_sections": [],
   "name": "U-Net scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
